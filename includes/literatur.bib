
@article{kim_ocr-free_2021,
	title = {{OCR}-free Document Understanding Transformer},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2111.15664},
	doi = {10.48550/ARXIV.2111.15664},
	abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding ({VDU}) methods outsource the task of reading text to off-the-shelf Optical Character Recognition ({OCR}) engines and focus on the understanding task with the {OCR} outputs. Although such {OCR}-based approaches have shown promising performance, they suffer from 1) high computational costs for using {OCR}; 2) inflexibility of {OCR} models on languages or types of document; 3) {OCR} error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel {OCR}-free {VDU} model named Donut, which stands for Document understanding transformer. As the first step in {OCR}-free {VDU} research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple {OCR}-free {VDU} model, Donut, achieves state-of-the-art performances on various {VDU} tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
	author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
	urldate = {2024-02-14},
	date = {2021},
	note = {Publisher: {arXiv}
Version Number: 5},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}
