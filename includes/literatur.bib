
@article{kim_ocr-free_2021,
	title = {{OCR}-free Document Understanding Transformer},
	url = {https://arxiv.org/abs/2111.15664},
	doi = {10.48550/ARXIV.2111.15664},
	abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding ({VDU}) methods outsource the task of reading text to off-the-shelf Optical Character Recognition ({OCR}) engines and focus on the understanding task with the {OCR} outputs. Although such {OCR}-based approaches have shown promising performance, they suffer from 1) high computational costs for using {OCR}; 2) inflexibility of {OCR} models on languages or types of document; 3) {OCR} error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel {OCR}-free {VDU} model named Donut, which stands for Document understanding transformer. As the first step in {OCR}-free {VDU} research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple {OCR}-free {VDU} model, Donut, achieves state-of-the-art performances on various {VDU} tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
	author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
	urldate = {2024-02-14},
	date = {2021},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@report{rackspace_most_2023,
	title = {Most popular {AI} use cases within enterprises worldwide in 2023},
	url = {https://www.statista.com/statistics/1447860/most-popular-ai-use-cases-enterprises/},
	pages = {1--22},
	number = {o. N.},
	institution = {Statista},
	author = {Rackspace},
	urldate = {2024-02-26},
	date = {2023-12-05},
	langid = {english},
}

@online{swoyer_state_2020,
	title = {The state of data quality in 2020},
	url = {https://www.oreilly.com/radar/the-state-of-data-quality-in-2020/},
	abstract = {O’Reilly survey highlights the increasing attention organizations are giving to data quality and how {AI} both exacerbates and alleviates data quality issues.},
	titleaddon = {O’Reilly Media},
	author = {Swoyer, Steve, Roger Magoulas},
	urldate = {2024-02-26},
	date = {2020-02-12},
	langid = {american},
	file = {Snapshot:/Users/novak/Zotero/storage/RPJ57EIK/the-state-of-data-quality-in-2020.html:text/html},
}

@online{howarth_57_2024,
	title = {57 {NEW} {AI} Statistics},
	url = {https://explodingtopics.com/blog/ai-statistics},
	abstract = {Explore insightful and up-to-date statistics on artificial intelligence ({AI}) including market size, growth, business use, job risks \& more.},
	titleaddon = {Exploding Topics},
	author = {Howarth, Josh},
	urldate = {2024-02-26},
	date = {2024-02-02},
	langid = {english},
	file = {Snapshot:/Users/novak/Zotero/storage/UTN29YL3/ai-statistics.html:text/html},
}

@inproceedings{esposito_intelligent_2005,
	location = {Seoul, South Korea},
	title = {Intelligent document processing},
	isbn = {978-0-7695-2420-7},
	url = {http://ieeexplore.ieee.org/document/1575714/},
	doi = {10.1109/ICDAR.2005.144},
	eventtitle = {Eighth International Conference on Document Analysis and Recognition ({ICDAR}'05)},
	pages = {1100--1104 Vol. 2},
	booktitle = {Eighth International Conference on Document Analysis and Recognition ({ICDAR}'05)},
	publisher = {{IEEE}},
	author = {Esposito, F. and Ferilli, S. and Basile, T.M.A. and Di Mauro, N.},
	urldate = {2024-02-26},
	date = {2005},
}

@report{dutt_now_2024,
	title = {Now decides next: Insights from the leading edge of generative {AI} adoptation},
	url = {https://www2.deloitte.com/de/de/pages/trends/ki-studie.html},
	pages = {1--33},
	institution = {Deloitte},
	type = {Study},
	author = {Dutt, Deboroshio and Ammanath, Beena and Perricos, Costi and Sniderman, Brenna},
	urldate = {2024-02-26},
	date = {2024-01},
	langid = {english},
}

@inproceedings{rahal_information_2018,
	location = {London},
	title = {Information Extraction from Arabic and Latin scanned invoices},
	isbn = {978-1-5386-1459-4},
	url = {https://ieeexplore.ieee.org/document/8480221/},
	doi = {10.1109/ASAR.2018.8480221},
	eventtitle = {2018 {IEEE} 2nd International Workshop on Arabic and Derived Script Analysis and Recognition ({ASAR})},
	pages = {145--150},
	booktitle = {2018 {IEEE} 2nd International Workshop on Arabic and Derived Script Analysis and Recognition ({ASAR})},
	publisher = {{IEEE}},
	author = {Rahal, Najoua and Tounsi, Maroua and Benjlaiel, Mohamed and Alimi, Adel M.},
	urldate = {2024-02-27},
	date = {2018-03},
}

@article{hamad_detailed_2016,
	title = {A Detailed Analysis of Optical Character Recognition Technology},
	volume = {4},
	issn = {2147-8228},
	url = {https://dergipark.org.tr/en/doi/10.18100/ijamec.270374},
	doi = {10.18100/ijamec.270374},
	pages = {244--244},
	issue = {Special Issue-1},
	journaltitle = {International Journal of Applied Mathematics, Electronics and Computers},
	author = {Hamad, Karez and Kaya, Mehmet},
	urldate = {2024-02-28},
	date = {2016-12-22},
	file = {Full Text:/Users/novak/Zotero/storage/WZYGAHXH/Hamad and Kaya - 2016 - A Detailed Analysis of Optical Character Recogniti.pdf:application/pdf},
}

@inproceedings{huang_layoutlmv3_2022,
	location = {Lisboa Portugal},
	title = {{LayoutLMv}3: Pre-training for Document {AI} with Unified Text and Image Masking},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3548112},
	doi = {10.1145/3503161.3548112},
	shorttitle = {{LayoutLMv}3},
	eventtitle = {{MM} '22: The 30th {ACM} International Conference on Multimedia},
	pages = {4083--4091},
	booktitle = {Proceedings of the 30th {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
	urldate = {2024-02-28},
	date = {2022-10-10},
	langid = {english},
	file = {Submitted Version:/Users/novak/Zotero/storage/59HRJTW4/Huang et al. - 2022 - LayoutLMv3 Pre-training for Document AI with Unif.pdf:application/pdf},
}

@misc{zhai_fast-structext_2023,
	title = {Fast-{StrucTexT}: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding},
	url = {http://arxiv.org/abs/2305.11392},
	shorttitle = {Fast-{StrucTexT}},
	abstract = {Transformers achieve promising performance in document understanding because of their high effectiveness and still suffer from quadratic computational complexity dependency on the sequence length. General efficient transformers are challenging to be directly adapted to model document. They are unable to handle the layout representation in documents, e.g. word, line and paragraph, on different granularity levels and seem hard to achieve a good trade-off between efficiency and performance. To tackle the concerns, we propose Fast-{StrucTexT}, an efficient multi-modal framework based on the {StrucTexT} algorithm with an hourglass transformer architecture, for visual document understanding. Specifically, we design a modality-guided dynamic token merging block to make the model learn multi-granularity representation and prunes redundant tokens. Additionally, we present a multi-modal interaction module called Symmetry Cross Attention ({SCA}) to consider multi-modal fusion and efficiently guide the token mergence. The {SCA} allows one modality input as query to calculate cross attention with another modality in a dual phase. Extensive experiments on {FUNSD}, {SROIE}, and {CORD} datasets demonstrate that our model achieves the state-of-the-art performance and almost 1.9X faster inference time than the state-of-the-art methods.},
	number = {{arXiv}:2305.11392},
	publisher = {{arXiv}},
	author = {Zhai, Mingliang and Li, Yulin and Qin, Xiameng and Yi, Chen and Xie, Qunyi and Zhang, Chengquan and Yao, Kun and Wu, Yuwei and Jia, Yunde},
	urldate = {2024-02-28},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.11392 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/novak/Zotero/storage/E5XPFQNX/Zhai et al. - 2023 - Fast-StrucTexT An Efficient Hourglass Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/novak/Zotero/storage/DBBUGBLK/2305.html:text/html},
}

@misc{peng_ernie-layout_2022,
	title = {{ERNIE}-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding},
	url = {http://arxiv.org/abs/2210.06155},
	shorttitle = {{ERNIE}-Layout},
	abstract = {Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematic mining and utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose {ERNIE}-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features from text, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that {ERNIE}-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at http://github.com/{PaddlePaddle}/{PaddleNLP}/tree/develop/model\_zoo/ernie-layout.},
	number = {{arXiv}:2210.06155},
	publisher = {{arXiv}},
	author = {Peng, Qiming and Pan, Yinxu and Wang, Wenjin and Luo, Bin and Zhang, Zhenyu and Huang, Zhengjie and Hu, Teng and Yin, Weichong and Chen, Yongfeng and Zhang, Yin and Feng, Shikun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
	urldate = {2024-02-28},
	date = {2022-10-14},
	eprinttype = {arxiv},
	eprint = {2210.06155 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/novak/Zotero/storage/MYESCDCK/Peng et al. - 2022 - ERNIE-Layout Layout Knowledge Enhanced Pre-traini.pdf:application/pdf;arXiv.org Snapshot:/Users/novak/Zotero/storage/PG3UQRJQ/2210.html:text/html},
}

@article{dongre_review_2010,
	title = {A Review of Research on Devnagari Character Recognition},
	volume = {12},
	issn = {09758887},
	url = {http://ijcaonline.org/volume12/number2/pxc3872224.pdf},
	doi = {10.5120/1653-2224},
	pages = {8--15},
	number = {2},
	journaltitle = {International Journal of Computer Applications},
	shortjournal = {{IJCA}},
	author = {Dongre, Vikas J and Mankar, Vijay H and Suganya, G},
	urldate = {2024-02-28},
	date = {2010-12-10},
	file = {Full Text:/Users/novak/Zotero/storage/GNUA59WX/Dongre et al. - 2010 - A Review of Research on Devnagari Character Recogn.pdf:application/pdf},
}

@article{kavzoglu_increasing_2009,
	title = {Increasing the accuracy of neural network classification using refined training data},
	volume = {24},
	issn = {13648152},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815208002156},
	doi = {10.1016/j.envsoft.2008.11.012},
	pages = {850--858},
	number = {7},
	journaltitle = {Environmental Modelling \& Software},
	shortjournal = {Environmental Modelling \& Software},
	author = {Kavzoglu, Taskin},
	urldate = {2024-02-29},
	date = {2009-07},
	langid = {english},
}

@inproceedings{nguyen_neural_2020,
	location = {Virtual Event China},
	title = {Neural Machine Translation with {BERT} for Post-{OCR} Error Detection and Correction},
	isbn = {978-1-4503-7585-6},
	url = {https://dl.acm.org/doi/10.1145/3383583.3398605},
	doi = {10.1145/3383583.3398605},
	eventtitle = {{JCDL} '20: The {ACM}/{IEEE} Joint Conference on Digital Libraries in 2020},
	pages = {333--336},
	booktitle = {Proceedings of the {ACM}/{IEEE} Joint Conference on Digital Libraries in 2020},
	publisher = {{ACM}},
	author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Nguyen, Nhu-Van and Coustaty, Mickael and Doucet, Antoine},
	urldate = {2024-02-29},
	date = {2020-08},
	langid = {english},
	file = {Submitted Version:/Users/novak/Zotero/storage/FHB9T3CE/Nguyen et al. - 2020 - Neural Machine Translation with BERT for Post-OCR .pdf:application/pdf},
}

@article{garncarek_lambert_2020,
	title = {{LAMBERT}: Layout-Aware (Language) Modeling for information extraction},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2002.08087},
	doi = {10.48550/ARXIV.2002.08087},
	shorttitle = {{LAMBERT}},
	abstract = {We introduce a simple new approach to the problem of understanding documents where non-trivial layout influences the local semantics. To this end, we modify the Transformer encoder architecture in a way that allows it to use layout features obtained from an {OCR} system, without the need to re-learn language semantics from scratch. We only augment the input of the model with the coordinates of token bounding boxes, avoiding, in this way, the use of raw images. This leads to a layout-aware language model which can then be fine-tuned on downstream tasks. The model is evaluated on an end-to-end information extraction task using four publicly available datasets: Kleister {NDA}, Kleister Charity, {SROIE} and {CORD}. We show that our model achieves superior performance on datasets consisting of visually rich documents, while also outperforming the baseline {RoBERTa} on documents with flat layout ({NDA} {\textbackslash}(F\_\{1\}{\textbackslash}) increase from 78.50 to 80.42). Our solution ranked first on the public leaderboard for the Key Information Extraction from the {SROIE} dataset, improving the {SOTA} {\textbackslash}(F\_\{1\}{\textbackslash})-score from 97.81 to 98.17.},
	author = {Garncarek, Łukasz and Powalski, Rafał and Stanisławek, Tomasz and Topolski, Bartosz and Halama, Piotr and Turski, Michał and Graliński, Filip},
	urldate = {2024-02-29},
	date = {2020},
	note = {Publisher: {arXiv}
Version Number: 5},
	keywords = {{FOS}: Computer and information sciences, Computation and Language (cs.{CL})},
}

@book{tunstall_natural_2022,
	location = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {First edition},
	title = {Natural language processing with Transformers: building language applications with Hugging Face},
	isbn = {978-1-09-810324-8},
	shorttitle = {Natural language processing with Transformers},
	pagetotal = {383},
	publisher = {O'Reilly},
	author = {Tunstall, Lewis and Werra, Leandro von and Wolf, Thomas and Géron, Aurélien},
	date = {2022},
	file = {Table of Contents PDF:/Users/novak/Zotero/storage/V2E2BQWM/Tunstall et al. - 2022 - Natural language processing with Transformers bui.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-03-01},
	date = {2017},
	note = {Publisher: [object Object]
Version Number: 7},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Computation and Language (cs.{CL})},
}

@online{buchholz_infographic_2023,
	title = {Infographic: Threads Shoots Past One Million User Mark at Lightning Speed},
	url = {https://www.statista.com/chart/29174/time-to-one-million-users},
	shorttitle = {Infographic},
	abstract = {This chart shows the time it took for selected online services to reach one million users.},
	titleaddon = {Statista Daily Data},
	author = {Buchholz, Katharina},
	urldate = {2024-03-01},
	date = {2023-07-07},
	langid = {english},
	file = {Snapshot:/Users/novak/Zotero/storage/NDCMXV42/time-to-one-million-users.html:text/html},
}

@book{beysolow_ii_applied_2018,
	location = {Berkeley, {CA}},
	title = {Applied Natural Language Processing with Python: Implementing Machine Learning and Deep Learning Algorithms for Natural Language Processing},
	isbn = {978-1-4842-3732-8 978-1-4842-3733-5},
	url = {http://link.springer.com/10.1007/978-1-4842-3733-5},
	shorttitle = {Applied Natural Language Processing with Python},
	publisher = {Apress},
	author = {Beysolow {II}, Taweh},
	urldate = {2024-03-01},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4842-3733-5},
	file = {Full Text:/Users/novak/Zotero/storage/3ELQBZ3H/Beysolow Ii - 2018 - Applied Natural Language Processing with Python I.pdf:application/pdf},
}

@article{popa_towards_2021,
	title = {Towards syntax-aware token embeddings},
	volume = {27},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/product/identifier/S1351324920000297/type/journal_article},
	doi = {10.1017/S1351324920000297},
	abstract = {Abstract
            
              Distributional semantic word representations are at the basis of most modern {NLP} systems. Their usefulness has been proven across various tasks, particularly as inputs to deep learning models. Beyond that, much work investigated fine-tuning the generic word embeddings to leverage linguistic knowledge from large lexical resources. Some work investigated context-dependent word token embeddings motivated by word sense disambiguation, using sequential context and large lexical resources. More recently, acknowledging the need for an in-context representation of words, some work leveraged information derived from language modelling and large amounts of data to induce contextualised representations. In this paper, we investigate
              S
              yntax-
              A
              ware word
              Tok
              en
              E
              mbeddings (
              {SATokE}
              ) as a way to explicitly encode specific information derived from the linguistic analysis of a sentence in vectors which are input to a deep learning model. We propose an efficient unsupervised learning algorithm based on tensor factorisation for computing these token embeddings given an arbitrary graph of linguistic structure. Applying this method to syntactic dependency structures, we investigate the usefulness of such token representations as part of deep learning models of text understanding. We encode a sentence either by learning embeddings for its tokens and the relations between them from scratch or by leveraging pre-trained relation embeddings to infer token representations. Given sufficient data, the former is slightly more accurate than the latter, yet both provide more informative token embeddings than standard word representations, even when the word representations have been learned on the same type of context from larger corpora (namely pre-trained dependency-based word embeddings). We use a large set of supervised tasks and two major deep learning families of models for sentence understanding to evaluate our proposal. We empirically demonstrate the superiority of the token representations compared to popular distributional representations of words for various sentence and sentence pair classification tasks.},
	pages = {691--720},
	number = {6},
	journaltitle = {Natural Language Engineering},
	shortjournal = {Nat. Lang. Eng.},
	author = {Popa, Diana Nicoleta and Perez, Julien and Henderson, James and Gaussier, Eric},
	urldate = {2024-03-01},
	date = {2021-11},
	langid = {english},
}
